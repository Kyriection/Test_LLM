/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Starting scriptStarting scriptStarting scriptStarting script



2025-01-23 02:05:17.708 | INFO     | __main__:main:27 - Global rank 0, local rank 0, device: 0
2025-01-23 02:05:17.711 | INFO     | __main__:main:30 - Process group initialized
2025-01-23 02:05:17.711 | INFO     | __main__:main:35 - Using dist with rank 0 (only rank 0 will log)
2025-01-23 02:05:17.711 | INFO     | __main__:main:36 - ****************************************
2025-01-23 02:05:17.711 | INFO     | __main__:main:37 - Starting training with the arguments
2025-01-23 02:05:17.711 | INFO     | __main__:main:39 - model_config                   configs/llama_1b.json
2025-01-23 02:05:17.711 | INFO     | __main__:main:39 - eval_every                     1000
2025-01-23 02:05:17.711 | INFO     | __main__:main:39 - save_every                     4000
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - dtype                          bfloat16
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - seed                           0
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - resume_step                    None
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - restore_optimizer              True
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - continue_from                  None
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - single_gpu                     False
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - save_dir                       /scratch-shared/HTJ/1b-model-baseline-fp4
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - use_hf_model                   False
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - workers                        16
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - batch_size                     64
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - gradient_accumulation          None
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - total_batch_size               512
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - lr                             0.0005
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - warmup_steps                   2000
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - num_training_steps             20000
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - max_train_tokens               None
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - optimizer                      adamw
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - max_length                     256
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - scheduler                      cosine
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - min_lr_ratio                   0.1
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - weight_decay                   0.0
2025-01-23 02:05:17.712 | INFO     | __main__:main:39 - grad_clipping                  0.0
2025-01-23 02:05:17.713 | INFO     | __main__:main:39 - activation_checkpointing       False
2025-01-23 02:05:17.713 | INFO     | __main__:main:39 - tags                           None
2025-01-23 02:05:17.713 | INFO     | __main__:main:39 - name                           1b-baseline-fp41e-3
2025-01-23 02:05:17.713 | INFO     | __main__:main:39 - project                        1bmodel
2025-01-23 02:05:17.713 | INFO     | __main__:main:39 - unset_wandb                    False
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - entity                         None
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - beta1                          0.9
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - beta2                          0.999
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - rank                           128
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - update_proj_gap                500
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - galore_scale                   1.0
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - proj_type                      std
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - proj_quant                     False
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - proj_bits                      8
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - proj_group_size                256
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - weight_quant                   True
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - weight_bits                    4
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - weight_group_size              256
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - stochastic_round               False
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - simulation                     True
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - cos_threshold                  1
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - gamma_proj                     2
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - queue_size                     5
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - act_quant                      False
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - act_bits                       8
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - act_group_size                 64
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - act_topk                       2
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - act_stochastic                 False
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - fp4                            True
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - gamma1                         0.85
2025-01-23 02:05:17.714 | INFO     | __main__:main:39 - gamma2                         0.999
2025-01-23 02:05:17.715 | INFO     | __main__:main:39 - theta                          0.999
2025-01-23 02:05:17.715 | INFO     | __main__:main:39 - total_T                        20000
2025-01-23 02:05:17.715 | INFO     | __main__:main:39 - eta                            0.5
2025-01-23 02:05:17.715 | INFO     | __main__:main:40 - ****************************************
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: tj-huang16 (dri-ice). Use `wandb login --relogin` to force relogin
2025-01-23 02:05:18.540 | INFO     | __main__:main:27 - Global rank 3, local rank 3, device: 3
2025-01-23 02:05:18.541 | INFO     | __main__:main:27 - Global rank 2, local rank 2, device: 2
2025-01-23 02:05:18.542 | INFO     | __main__:main:30 - Process group initialized
2025-01-23 02:05:18.544 | INFO     | __main__:main:30 - Process group initialized
2025-01-23 02:05:18.544 | INFO     | __main__:main:27 - Global rank 1, local rank 1, device: 1
2025-01-23 02:05:18.546 | INFO     | __main__:main:30 - Process group initialized
wandb: Tracking run with wandb version 0.18.5
wandb: Run data is saved locally in /gpfs/home2/huangti/Projects/LLM_Pretraining_Test/wandb/run-20250123_020518-vs81qvxy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 1b-baseline-fp41e-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/dri-ice/1bmodel
wandb: üöÄ View run at https://wandb.ai/dri-ice/1bmodel/runs/vs81qvxy
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation
  warnings.warn(
2025-01-23 02:05:25.794 | INFO     | utils.dataloader:setup_dataset:20 - Shuffling data with seed 42
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation
  warnings.warn(
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation
  warnings.warn(
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation
  warnings.warn(
FP4 training
Keep in original linear layer lm_head Linear(in_features=2048, out_features=32000, bias=False)
FP4 training
Keep in original linear layer lm_head Linear(in_features=2048, out_features=32000, bias=False)
FP4 training
Keep in original linear layer lm_head Linear(in_features=2048, out_features=32000, bias=False)
FP4 training
Keep in original linear layer lm_head Linear(in_features=2048, out_features=32000, bias=False)
2025-01-23 02:05:44.094 | INFO     | utils.setup:setup_model:66 - ----------------------------------------
2025-01-23 02:05:44.095 | INFO     | utils.setup:setup_model:67 - Prepare Model for Activation&Weight FP4 Training
2025-01-23 02:05:44.095 | INFO     | utils.setup:setup_model:68 - ----------------------------------------
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
Update steps:   0%|                                   | 0/20000 [00:00<?, ?it/s]2025-01-23 02:05:44.152 | INFO     | __main__:main:169 - 
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 2048, padding_idx=31999)
    (layers): ModuleList(
      (0-23): 24 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Qfp4Linear(in_features=2048, out_features=2048, bias=False)
          (k_proj): Qfp4Linear(in_features=2048, out_features=2048, bias=False)
          (v_proj): Qfp4Linear(in_features=2048, out_features=2048, bias=False)
          (o_proj): Qfp4Linear(in_features=2048, out_features=2048, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Qfp4Linear(in_features=2048, out_features=5461, bias=False)
          (down_proj): Qfp4Linear(in_features=5461, out_features=2048, bias=False)
          (up_proj): Qfp4Linear(in_features=2048, out_features=5461, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)
)

2025-01-23 02:05:44.153 | INFO     | __main__:main:170 - Total params: 1339.08M
2025-01-23 02:05:44.153 | INFO     | __main__:main:177 - Trainable params: 1339.08M
2025-01-23 02:05:44.153 | INFO     | __main__:main:183 - Saving model to /scratch-shared/HTJ/1b-model-baseline-fp4 every 4000 update steps
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Update steps:   0%|                       | 1/20000 [01:04<357:57:15, 64.44s/it]Traceback (most recent call last):
  File "/gpfs/home2/huangti/Projects/LLM_Pretraining_Test/main_pretrain.py", line 428, in <module>
    main(args)
  File "/gpfs/home2/huangti/Projects/LLM_Pretraining_Test/main_pretrain.py", line 357, in main
    "grad_norm_afterclip":global_grad_norm_after.item(),
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'float' object has no attribute 'item'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/gpfs/home2/huangti/Projects/LLM_Pretraining_Test/main_pretrain.py", line 428, in <module>
[rank0]:     main(args)
[rank0]:   File "/gpfs/home2/huangti/Projects/LLM_Pretraining_Test/main_pretrain.py", line 357, in main
[rank0]:     "grad_norm_afterclip":global_grad_norm_after.item(),
[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: AttributeError: 'float' object has no attribute 'item'
[1;34mwandb[0m: üöÄ View run [33m1b-baseline-fp41e-3[0m at: [34mhttps://wandb.ai/dri-ice/1bmodel/runs/vs81qvxy[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250123_020518-vs81qvxy/logs[0m
W0123 02:06:51.987000 1172085 /gpfs/home2/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1172106 closing signal SIGTERM
W0123 02:06:51.991000 1172085 /gpfs/home2/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1172107 closing signal SIGTERM
W0123 02:06:52.002000 1172085 /gpfs/home2/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1172108 closing signal SIGTERM
E0123 02:06:53.707000 1172085 /gpfs/home2/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 1172105) of binary: /home/huangti/miniconda3/envs/spam/bin/python
Traceback (most recent call last):
  File "/home/huangti/miniconda3/envs/spam/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.5.0', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main_pretrain.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-01-23_02:06:51
  host      : gcn144.local.snellius.surf.nl
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1172105)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

JOB STATISTICS
==============
Job ID: 9564080
Cluster: snellius
User/Group: huangti/huangti
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 64
CPU Utilized: 00:07:49
CPU Efficiency: 5.43% of 02:24:00 core-walltime
Job Wall-clock time: 00:02:15
Memory Utilized: 27.16 GB
Memory Efficiency: 3.77% of 720.00 GB
