/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Starting script
Starting script
2025-01-22 03:22:30.431 | INFO     | __main__:main:27 - Global rank 0, local rank 0, device: 0
2025-01-22 03:22:30.433 | INFO     | __main__:main:30 - Process group initialized
2025-01-22 03:22:30.433 | INFO     | __main__:main:35 - Using dist with rank 0 (only rank 0 will log)
2025-01-22 03:22:30.433 | INFO     | __main__:main:36 - ****************************************
2025-01-22 03:22:30.433 | INFO     | __main__:main:37 - Starting training with the arguments
2025-01-22 03:22:30.433 | INFO     | __main__:main:39 - model_config                   configs/llama_130m.json
2025-01-22 03:22:30.433 | INFO     | __main__:main:39 - eval_every                     1000
2025-01-22 03:22:30.433 | INFO     | __main__:main:39 - save_every                     100000
2025-01-22 03:22:30.433 | INFO     | __main__:main:39 - dtype                          bfloat16
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - seed                           0
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - resume_step                    None
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - restore_optimizer              True
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - continue_from                  None
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - single_gpu                     False
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - save_dir                       /scratch-shared/HTJ/llama_130m_adamw_qweight_simulate
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - use_hf_model                   False
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - workers                        16
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - batch_size                     256
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - gradient_accumulation          None
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - total_batch_size               512
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - lr                             0.003
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - warmup_steps                   1000
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - num_training_steps             10000
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - max_train_tokens               None
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - optimizer                      adammini
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - max_length                     256
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - scheduler                      cosine
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - min_lr_ratio                   0.1
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - weight_decay                   0.0
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - grad_clipping                  0.0
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - activation_checkpointing       False
2025-01-22 03:22:30.434 | INFO     | __main__:main:39 - tags                           None
2025-01-22 03:22:30.435 | INFO     | __main__:main:39 - name                           130m-adamin-3e-3q4
2025-01-22 03:22:30.435 | INFO     | __main__:main:39 - project                        galore-x
2025-01-22 03:22:30.435 | INFO     | __main__:main:39 - unset_wandb                    False
2025-01-22 03:22:30.435 | INFO     | __main__:main:39 - entity                         None
2025-01-22 03:22:30.435 | INFO     | __main__:main:39 - beta1                          0.9
2025-01-22 03:22:30.435 | INFO     | __main__:main:39 - beta2                          0.999
2025-01-22 03:22:30.435 | INFO     | __main__:main:39 - rank                           128
2025-01-22 03:22:30.435 | INFO     | __main__:main:39 - update_proj_gap                500
2025-01-22 03:22:30.435 | INFO     | __main__:main:39 - galore_scale                   1.0
2025-01-22 03:22:30.435 | INFO     | __main__:main:39 - proj_type                      std
2025-01-22 03:22:30.435 | INFO     | __main__:main:39 - proj_quant                     False
2025-01-22 03:22:30.435 | INFO     | __main__:main:39 - proj_bits                      8
2025-01-22 03:22:30.435 | INFO     | __main__:main:39 - proj_group_size                256
2025-01-22 03:22:30.435 | INFO     | __main__:main:39 - weight_quant                   True
2025-01-22 03:22:30.435 | INFO     | __main__:main:39 - weight_bits                    4
2025-01-22 03:22:30.435 | INFO     | __main__:main:39 - weight_group_size              256
2025-01-22 03:22:30.436 | INFO     | __main__:main:39 - stochastic_round               False
2025-01-22 03:22:30.436 | INFO     | __main__:main:39 - simulation                     True
2025-01-22 03:22:30.436 | INFO     | __main__:main:39 - cos_threshold                  1
2025-01-22 03:22:30.436 | INFO     | __main__:main:39 - gamma_proj                     2
2025-01-22 03:22:30.436 | INFO     | __main__:main:39 - queue_size                     5
2025-01-22 03:22:30.436 | INFO     | __main__:main:39 - act_quant                      True
2025-01-22 03:22:30.436 | INFO     | __main__:main:39 - act_bits                       8
2025-01-22 03:22:30.436 | INFO     | __main__:main:39 - act_group_size                 64
2025-01-22 03:22:30.436 | INFO     | __main__:main:39 - act_topk                       2
2025-01-22 03:22:30.436 | INFO     | __main__:main:39 - act_stochastic                 True
2025-01-22 03:22:30.436 | INFO     | __main__:main:39 - fp4                            False
2025-01-22 03:22:30.436 | INFO     | __main__:main:39 - gamma1                         0.9
2025-01-22 03:22:30.436 | INFO     | __main__:main:39 - gamma2                         0.999
2025-01-22 03:22:30.436 | INFO     | __main__:main:39 - theta                          0.999
2025-01-22 03:22:30.436 | INFO     | __main__:main:39 - total_T                        20000
2025-01-22 03:22:30.436 | INFO     | __main__:main:39 - eta                            0.5
2025-01-22 03:22:30.436 | INFO     | __main__:main:40 - ****************************************
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
2025-01-22 03:22:30.638 | INFO     | __main__:main:27 - Global rank 1, local rank 1, device: 1
2025-01-22 03:22:30.640 | INFO     | __main__:main:30 - Process group initialized
wandb: Currently logged in as: tj-huang16 (dri-ice). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init().../home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation
  warnings.warn(
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...Activation-Weight Quantizing
Keep in original linear layer lm_head Linear(in_features=768, out_features=32000, bias=False)
wandb: | Waiting for wandb.init()...Warning by Adam-mini: model_sharding is deprecated since version 1.0.2. This argument is always set True. We will remove this argument in the future version.
Adam-mini found the param block with name: model.embed_tokens.weight torch.Size([32000, 768])
Adam-mini found the param block with name: model.layers.0.self_attn.q_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.0.self_attn.k_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.0.self_attn.v_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.0.self_attn.o_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.0.mlp.gate_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.0.mlp.down_proj.weight torch.Size([768, 2048])
Adam-mini found the param block with name: model.layers.0.mlp.up_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.0.input_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.0.post_attention_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.1.self_attn.q_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.1.self_attn.k_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.1.self_attn.v_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.1.self_attn.o_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.1.mlp.gate_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.1.mlp.down_proj.weight torch.Size([768, 2048])
Adam-mini found the param block with name: model.layers.1.mlp.up_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.1.input_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.1.post_attention_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.2.self_attn.q_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.2.self_attn.k_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.2.self_attn.v_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.2.self_attn.o_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.2.mlp.gate_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.2.mlp.down_proj.weight torch.Size([768, 2048])
Adam-mini found the param block with name: model.layers.2.mlp.up_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.2.input_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.2.post_attention_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.3.self_attn.q_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.3.self_attn.k_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.3.self_attn.v_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.3.self_attn.o_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.3.mlp.gate_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.3.mlp.down_proj.weight torch.Size([768, 2048])
Adam-mini found the param block with name: model.layers.3.mlp.up_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.3.input_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.3.post_attention_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.4.self_attn.q_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.4.self_attn.k_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.4.self_attn.v_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.4.self_attn.o_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.4.mlp.gate_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.4.mlp.down_proj.weight torch.Size([768, 2048])
Adam-mini found the param block with name: model.layers.4.mlp.up_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.4.input_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.4.post_attention_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.5.self_attn.q_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.5.self_attn.k_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.5.self_attn.v_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.5.self_attn.o_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.5.mlp.gate_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.5.mlp.down_proj.weight torch.Size([768, 2048])
Adam-mini found the param block with name: model.layers.5.mlp.up_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.5.input_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.5.post_attention_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.6.self_attn.q_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.6.self_attn.k_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.6.self_attn.v_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.6.self_attn.o_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.6.mlp.gate_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.6.mlp.down_proj.weight torch.Size([768, 2048])
Adam-mini found the param block with name: model.layers.6.mlp.up_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.6.input_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.6.post_attention_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.7.self_attn.q_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.7.self_attn.k_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.7.self_attn.v_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.7.self_attn.o_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.7.mlp.gate_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.7.mlp.down_proj.weight torch.Size([768, 2048])
Adam-mini found the param block with name: model.layers.7.mlp.up_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.7.input_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.7.post_attention_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.8.self_attn.q_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.8.self_attn.k_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.8.self_attn.v_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.8.self_attn.o_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.8.mlp.gate_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.8.mlp.down_proj.weight torch.Size([768, 2048])
Adam-mini found the param block with name: model.layers.8.mlp.up_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.8.input_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.8.post_attention_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.9.self_attn.q_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.9.self_attn.k_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.9.self_attn.v_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.9.self_attn.o_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.9.mlp.gate_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.9.mlp.down_proj.weight torch.Size([768, 2048])
Adam-mini found the param block with name: model.layers.9.mlp.up_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.9.input_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.9.post_attention_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.10.self_attn.q_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.10.self_attn.k_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.10.self_attn.v_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.10.self_attn.o_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.10.mlp.gate_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.10.mlp.down_proj.weight torch.Size([768, 2048])
Adam-mini found the param block with name: model.layers.10.mlp.up_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.10.input_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.10.post_attention_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.11.self_attn.q_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.11.self_attn.k_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.11.self_attn.v_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.11.self_attn.o_proj.weight torch.Size([768, 768])
Adam-mini found the param block with name: model.layers.11.mlp.gate_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.11.mlp.down_proj.weight torch.Size([768, 2048])
Adam-mini found the param block with name: model.layers.11.mlp.up_proj.weight torch.Size([2048, 768])
Adam-mini found the param block with name: model.layers.11.input_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.layers.11.post_attention_layernorm.weight torch.Size([768])
Adam-mini found the param block with name: model.norm.weight torch.Size([768])
Adam-mini found the param block with name: lm_head.weight torch.Size([32000, 768])
wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: ERROR Run initialization has timed out after 90.0 sec. 
wandb: ERROR Please refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-
Traceback (most recent call last):
  File "/gpfs/home2/huangti/Projects/LLM_Pretraining_Test/main_pretrain.py", line 418, in <module>
    main(args)
  File "/gpfs/home2/huangti/Projects/LLM_Pretraining_Test/main_pretrain.py", line 44, in main
    wandb.init(project=args.project, name=args.name, entity=args.entity)
  File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/wandb/sdk/wandb_init.py", line 1270, in init
    wandb._sentry.reraise(e)
  File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/wandb/analytics/sentry.py", line 161, in reraise
    raise exc.with_traceback(sys.exc_info()[2])
  File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/wandb/sdk/wandb_init.py", line 1256, in init
    return wi.init()
           ^^^^^^^^^
  File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/wandb/sdk/wandb_init.py", line 848, in init
    raise error
wandb.errors.errors.CommError: Run initialization has timed out after 90.0 sec. 
Please refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-
[rank0]: Traceback (most recent call last):
[rank0]:   File "/gpfs/home2/huangti/Projects/LLM_Pretraining_Test/main_pretrain.py", line 418, in <module>
[rank0]:     main(args)
[rank0]:   File "/gpfs/home2/huangti/Projects/LLM_Pretraining_Test/main_pretrain.py", line 44, in main
[rank0]:     wandb.init(project=args.project, name=args.name, entity=args.entity)
[rank0]:   File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/wandb/sdk/wandb_init.py", line 1270, in init
[rank0]:     wandb._sentry.reraise(e)
[rank0]:   File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/wandb/analytics/sentry.py", line 161, in reraise
[rank0]:     raise exc.with_traceback(sys.exc_info()[2])
[rank0]:   File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/wandb/sdk/wandb_init.py", line 1256, in init
[rank0]:     return wi.init()
[rank0]:            ^^^^^^^^^
[rank0]:   File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/wandb/sdk/wandb_init.py", line 848, in init
[rank0]:     raise error
[rank0]: wandb.errors.errors.CommError: Run initialization has timed out after 90.0 sec. 
[rank0]: Please refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-
[rank0]:[W122 03:24:07.517333097 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0122 03:24:08.065000 2376234 /gpfs/home2/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2376238 closing signal SIGTERM
E0122 03:24:08.430000 2376234 /gpfs/home2/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 2376237) of binary: /home/huangti/miniconda3/envs/spam/bin/python
Traceback (most recent call last):
  File "/home/huangti/miniconda3/envs/spam/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.5.0', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main_pretrain.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-01-22_03:24:08
  host      : gcn134.local.snellius.surf.nl
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2376237)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

JOB STATISTICS
==============
Job ID: 9544791
Cluster: snellius
User/Group: huangti/huangti
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 32
CPU Utilized: 00:00:52
CPU Efficiency: 1.34% of 01:04:32 core-walltime
Job Wall-clock time: 00:02:01
Memory Utilized: 1.79 GB
Memory Efficiency: 0.50% of 360.00 GB
